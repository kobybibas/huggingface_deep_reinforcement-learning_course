{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kobybibas/huggingface_deep_reinforcement-learning_course/blob/main/unit8_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cf5-oDPjwf8"
      },
      "source": [
        "# Unit 8: Proximal Policy Gradient (PPO) with PyTorch ü§ñ\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/thumbnail.png\" alt=\"Unit 8\"/>\n",
        "\n",
        "\n",
        "In this notebook, you'll learn to **code your PPO agent from scratch with PyTorch using CleanRL implementation as model**.\n",
        "\n",
        "To test its robustness, we're going to train it in:\n",
        "\n",
        "- [LunarLander-v2 üöÄ](https://www.gymlibrary.dev/environments/box2d/lunar_lander/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Fl6Rxt0lc0O"
      },
      "source": [
        "‚¨áÔ∏è Here is an example of what you will achieve. ‚¨áÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbKfCj5ilgqT"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<video controls autoplay><source src=\"https://huggingface.co/sb3/ppo-LunarLander-v2/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcOFdWpnlxNf"
      },
      "source": [
        "We're constantly trying to improve our tutorials, so **if you find some issues in this notebook**, please [open an issue on the GitHub Repo](https://github.com/huggingface/deep-rl-class/issues)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objectives of this notebook üèÜ\n",
        "\n",
        "At the end of the notebook, you will:\n",
        "\n",
        "- Be able to **code your PPO agent from scratch using PyTorch**.\n",
        "- Be able to **push your trained agent and the code to the Hub** with a nice video replay and an evaluation score üî•.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T6lIPYFghhYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This notebook is from the Deep Reinforcement Learning Course\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>\n",
        "\n",
        "In this free course, you will:\n",
        "\n",
        "- üìñ Study Deep Reinforcement Learning in **theory and practice**.\n",
        "- üßë‚Äçüíª Learn to **use famous Deep RL libraries** such as Stable Baselines3, RL Baselines3 Zoo, CleanRL and Sample Factory 2.0.\n",
        "- ü§ñ Train **agents in unique environments**\n",
        "\n",
        "Don‚Äôt forget to **<a href=\"http://eepurl.com/ic5ZUD\">sign up to the course</a>** (we are collecting your email to be able to¬†**send you the links when each Unit is published and give you information about the challenges and updates).**\n",
        "\n",
        "\n",
        "The best way to keep in touch is to join our discord server to exchange with the community and with us üëâüèª https://discord.gg/ydHrjt3WP5"
      ],
      "metadata": {
        "id": "Wp-rD6Fuhq31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites üèóÔ∏è\n",
        "Before diving into the notebook, you need to:\n",
        "\n",
        "üî≤ üìö Study [PPO by reading Unit 8](https://huggingface.co/deep-rl-course/unit8/introduction) ü§ó  "
      ],
      "metadata": {
        "id": "rasqqGQlhujA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process), you need to push one model, we don't ask for a minimal result but we **advise you to try different hyperparameters settings to get better results**.\n",
        "\n",
        "If you don't find your model, **go to the bottom of the page and click on the refresh button**\n",
        "\n",
        "For more information about the certification process, check this section üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
      ],
      "metadata": {
        "id": "PUFfMGOih3CW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set the GPU üí™\n",
        "- To **accelerate the agent's training, we'll use a GPU**. To do that, go to `Runtime > Change Runtime type`\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Step 1\">"
      ],
      "metadata": {
        "id": "PU4FVzaoM6fC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `Hardware Accelerator > GPU`\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" alt=\"GPU Step 2\">"
      ],
      "metadata": {
        "id": "KV0NyFdQM9ZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a virtual display üîΩ\n",
        "\n",
        "During the notebook, we'll need to generate a replay video. To do so, with colab, **we need to have a virtual screen to be able to render the environment** (and thus record the frames).\n",
        "\n",
        "Hence the following cell will install the librairies and create and run a virtual screen üñ•"
      ],
      "metadata": {
        "id": "bTpYcVZVMzUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install setuptools==65.5.0"
      ],
      "metadata": {
        "id": "Fd731S8-NuJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jV6wjQ7Be7p5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt install python-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!apt install swig cmake\n",
        "!pip install pyglet==1.5\n",
        "!pip3 install pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ],
      "metadata": {
        "id": "ww5PQH1gNLI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncIgfNf3mOtc"
      },
      "source": [
        "## Install dependencies üîΩ\n",
        "For this exercise, we use `gym==0.22`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install gym==0.22\n",
        "# !pip install gym[box2d]==0.22\n",
        "\n",
        "!pip install imageio-ffmpeg\n",
        "!pip install huggingface_hub\n",
        "!pip install gymnasium\n",
        "!pip install gymnasium[box2d]\n",
        "!pip install -q imageio[ffmpeg]\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "9xZQFTPcsKUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDkUufewmq6v"
      },
      "source": [
        "## Let's code PPO from scratch with Costa Huang tutorial\n",
        "- For the core implementation of PPO we're going to use the excellent [Costa Huang](https://costa.sh/) tutorial.\n",
        "- In addition to the tutorial, to go deeper you can read the 37 core implementation details: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/\n",
        "\n",
        "üëâ The video tutorial: https://youtu.be/MEt6rrxH8W4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNgEL1_uvhaq"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/MEt6rrxH8W4\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f34ILn7AvTbt"
      },
      "source": [
        "- The best is to code first on the cell below, this way, if you kill the machine **you don't loose the implementation**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bE708C6mhE7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "def make_env(num_envs):\n",
        "    envs = gym.make_vec(\"LunarLander-v3\", num_envs=num_envs, vectorization_mode=\"sync\")\n",
        "    envs.reset(seed=123)\n",
        "    return envs\n",
        "\n",
        "num_envs = 10\n",
        "envs = make_env(num_envs=num_envs)\n",
        "\n",
        "print(\"Action space:\", envs.action_space)\n",
        "print(\"Sample action:\", envs.action_space.sample())\n",
        "obs, info = envs.reset(seed=123)\n",
        "print(\"Observation shape:\", obs.shape)  # Expect (3, 8) for 3 envs\n",
        "num_actions = 4\n",
        "input_size = num_observations = 8 # Observation shape\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, num_actions):\n",
        "        super().__init__()\n",
        "        self.shared_layers = nn.Sequential(\n",
        "          nn.Linear(input_size, 128),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(128, 64),\n",
        "        nn.ReLU(),\n",
        "        )\n",
        "        self.value_head =  nn.Linear(64, 1)\n",
        "        self.policy_head =  nn.Linear(64, num_actions)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.shared_layers(x)\n",
        "        logits = self.policy_head(x)\n",
        "        value = self.value_head(x)\n",
        "        return logits, value\n",
        "\n",
        "\n",
        "model = ActorCritic(input_size, num_actions)\n",
        "model = model.to('cuda')\n",
        "obs_tensor = torch.tensor(obs)\n",
        "logits, value = model(obs_tensor)\n",
        "print(\"Logits shape:\", logits.shape)  # Expect (3, 4)\n",
        "print(\"Value shape:\", value.shape)    # Expect (3, 1)"
      ],
      "metadata": {
        "id": "lNqpEPUpl9eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "\n",
        "def collect_rollout(model, envs, n_steps):\n",
        "    with torch.no_grad():\n",
        "        memory = []\n",
        "        obs, info = envs.reset(seed=123)\n",
        "        for _ in tqdm(range(n_steps)):\n",
        "            obs_tensor = torch.tensor(obs, dtype=torch.float32).to('cuda')\n",
        "            logits, values = model(obs_tensor)\n",
        "            dist = Categorical(logits=logits)\n",
        "            actions = dist.sample()\n",
        "            logprobs = dist.log_prob(actions)\n",
        "            actions_np = actions.cpu().numpy().astype(np.int32)\n",
        "            # print(actions_np)\n",
        "\n",
        "            next_obs, rewards, terminations, truncations, infos = envs.step(actions_np)\n",
        "\n",
        "            memory.append({\n",
        "                'obs': obs_tensor,\n",
        "                'action': actions,\n",
        "                'logprob': logprobs,\n",
        "                'reward': torch.tensor(rewards, dtype=torch.float32).to('cuda'),\n",
        "                'done': torch.tensor(terminations, dtype=torch.float32).to('cuda'),\n",
        "                'value': values\n",
        "            })\n",
        "\n",
        "            obs = next_obs\n",
        "    return memory\n",
        "envs = make_env(num_envs)\n",
        "n_steps = 100000\n",
        "memory = collect_rollout(model, envs, n_steps)\n",
        "# print(memory)"
      ],
      "metadata": {
        "id": "Ql00NpJfo0Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute GAE (Generalized Advantage Estimation)\n",
        "def compute_gae(memory, gamma, lam):\n",
        "    n_step = len(memory)\n",
        "    num_envs = memory[0]['value'].shape[0]\n",
        "    advantages = torch.zeros((n_step, num_envs)).to('cuda')\n",
        "\n",
        "    adv_t_plus_1 = torch.zeros(num_envs).to('cuda')\n",
        "    memory_t_plus_1_value = torch.zeros(num_envs).to('cuda')\n",
        "\n",
        "    for t in reversed(range(n_step)):\n",
        "        memory_t = memory[t]\n",
        "        r_t = memory_t['reward']                      # shape: [num_envs]\n",
        "        done_t = memory_t['done']                     # shape: [num_envs]\n",
        "        v_t = memory_t['value'].squeeze(-1)           # shape: [num_envs]\n",
        "\n",
        "        # print( r_t, gamma, memory_t_plus_1_value, (1 - done_t), v_t)\n",
        "        delta_t = r_t + gamma * memory_t_plus_1_value * (1 - done_t) - v_t\n",
        "        adv_t = delta_t + gamma * lam * (1 - done_t) * adv_t_plus_1\n",
        "\n",
        "        advantages[t] = adv_t.detach()\n",
        "        adv_t_plus_1 = adv_t\n",
        "        memory_t_plus_1_value = v_t\n",
        "\n",
        "    values = torch.stack([m['value'].squeeze(-1) for m in memory])  # shape: [n_step, num_envs]\n",
        "    return advantages + values\n",
        "\n",
        "\n",
        "\n",
        "gamma = 0.1\n",
        "lam = 0.1\n",
        "gae = compute_gae(memory, gamma, lam)\n",
        "print(gae, gae.shape)"
      ],
      "metadata": {
        "id": "Z7p6Wb4pwcyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "batch_size = 2048\n",
        "lr = 0.001\n",
        "epoch = 100\n",
        "\n",
        "# PPO Loss and Policy Update\n",
        "def compute_ppo_loss(obs, values_predicted, logprobs_new, logprobs_old, returns, advantages, entropy, epsilon=0.2, c1=0.5, c2=0.02):\n",
        "    r_t = torch.exp(logprobs_new - logprobs_old)\n",
        "    loss_ppo = -torch.minimum(r_t*advantages, torch.clamp(r_t, 1-epsilon, 1+epsilon) * advantages)\n",
        "    loss_value = nn.functional.mse_loss(values_predicted, returns)\n",
        "    entropy_bonus= entropy\n",
        "    return loss_ppo.mean() + c1 * loss_value - c2 * entropy_bonus\n",
        "\n",
        "\n",
        "model = ActorCritic(input_size, num_actions)\n",
        "model.train()\n",
        "optimizer = Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Stack rollout data to tensorts\n",
        "obs_all = torch.stack([m['obs'] for m in memory]).reshape(-1,num_observations)\n",
        "logprobs_old_all = torch.stack([m['logprob'] for m in memory]).reshape(-1,1)\n",
        "returns_all = torch.stack([m['reward'] for m in memory]).reshape(-1,1)\n",
        "actions_all = torch.stack([m['action'] for m in memory]).reshape(-1, 1)\n",
        "advantages_all = gae.reshape(-1,1)\n",
        "\n",
        "# Move to gpu\n",
        "model = model.to('cuda')\n",
        "obs_all = obs_all.to('cuda')\n",
        "logprobs_old_all = logprobs_old_all.to('cuda')\n",
        "returns_all = returns_all.to('cuda')\n",
        "actions_all = actions_all.to('cuda')\n",
        "advantages_all = advantages_all.to('cuda')\n",
        "\n",
        "\n",
        "print(f'{obs_all.shape, logprobs_old_all.shape, returns_all.shape, actions_all.shape,  advantages_all.shape}')\n",
        "dataset = TensorDataset(obs_all, logprobs_old_all, returns_all, actions_all,  advantages_all)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "loss_tracker = []\n",
        "for epoch in tqdm(range(epoch)):\n",
        "\n",
        "    loss_tracker_batch = []\n",
        "    for obs, logprobs_old, returns, actions, advantages in dataloader:\n",
        "\n",
        "        # Feed obs through your current policy network to get the updated logprob_new\n",
        "        logits, values_predicted = model(obs)\n",
        "        dist = Categorical(logits=logits)\n",
        "        logprobs_new = dist.log_prob(actions.squeeze(-1)).unsqueeze(1)  # using the actions we logged but with the new probability the model assigns\n",
        "        entropy = dist.entropy().mean()\n",
        "        # print(f'{[obs.shape, logprobs_new.shape, logprobs_old.shape, returns.shape, obs.shape, advantages.shape]}')\n",
        "        # print(f'{[obs, logprobs_new, logprobs_old, returns, obs, advantages]}')\n",
        "\n",
        "        loss = compute_ppo_loss(obs, values_predicted, logprobs_new, logprobs_old, returns, advantages, entropy)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_tracker_batch.append(loss.item())\n",
        "    loss_tracker.append(np.mean(loss_tracker_batch))"
      ],
      "metadata": {
        "id": "h8HvaZ0htLED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss_tracker)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9TUCc2mu_v6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eval\n",
        "def evaluate_agent(model, env_name=\"LunarLander-v3\", episodes=5, render=True):\n",
        "    env = gym.make(env_name)\n",
        "    model.eval()\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        obs, _ = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)  # add batch dim\n",
        "            obs_tensor = obs_tensor.to('cuda')\n",
        "            # print(obs_tensor.shape)\n",
        "            with torch.no_grad():\n",
        "                logits, _ = model(obs_tensor)\n",
        "                dist = Categorical(logits=logits)\n",
        "                action = dist.probs.argmax(dim=-1).item()  # Greedy action (no sampling)\n",
        "\n",
        "            obs, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "\n",
        "            if render:\n",
        "                env.render()  # This will open a window locally or show animation in notebook\n",
        "\n",
        "        print(f\"Episode {ep+1} finished with reward: {total_reward}\")\n",
        "\n",
        "    env.close()\n",
        "evaluate_agent(model, env_name=\"LunarLander-v3\", episodes=5, render=True)"
      ],
      "metadata": {
        "id": "xAdrwGDv9DqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "import os\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "def evaluate_and_save_video(model, env_name=\"LunarLander-v3\", filename=\"lander.mp4\", max_frames=1000):\n",
        "    import gymnasium as gym\n",
        "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "    obs, _ = env.reset()\n",
        "    model.eval()\n",
        "\n",
        "    frames = []\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    for _ in range(max_frames):\n",
        "        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to('cuda')\n",
        "        with torch.no_grad():\n",
        "            logits, _ = model(obs_tensor)\n",
        "            dist = Categorical(logits=logits)\n",
        "            action = dist.probs.argmax(dim=-1).item()\n",
        "\n",
        "        obs, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    env.close()\n",
        "    print(f\"Episode reward: {total_reward}\")\n",
        "\n",
        "    # Save video\n",
        "    imageio.mimsave(filename, frames, fps=30)\n",
        "\n",
        "def show_video(filename):\n",
        "    mp4 = open(filename,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    return HTML(f'<video width=480 controls><source src=\"{data_url}\" type=\"video/mp4\"></video>')\n",
        "\n",
        "# Usage\n",
        "evaluate_and_save_video(model)\n",
        "show_video(\"lander.mp4\")\n"
      ],
      "metadata": {
        "id": "jdx-6M6i9HJn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}